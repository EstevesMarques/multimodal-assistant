# Multimodal Assistant ğŸ¥—

**Assistente Nutricional Multimodal com IA**

NMultimodal Assistant Ã© uma aplicaÃ§Ã£o web inteligente construÃ­da com Streamlit e LangChain que utiliza o poder dos modelos de linguagem multimodais (como o GPT-4o da OpenAI) para analisar imagens de alimentos.

Basta tirar uma foto ou enviar uma imagem de um prato, fazer uma pergunta, e a IA fornecerÃ¡ uma anÃ¡lise detalhada sobre calorias estimadas, ingredientes e informaÃ§Ãµes nutricionais.

![Demo do Multimodal Assistant](https://iaplaybook.tech/images/posts/multimodal-assistant-demo.gif) 


## âœ¨ Funcionalidades Principais

-   **AnÃ¡lise por Imagem:** Envie um arquivo de imagem (`jpg`, `jpeg`, `png`) para anÃ¡lise.
-   **Captura com a CÃ¢mera:** Tire uma foto do seu prato em tempo real diretamente pela aplicaÃ§Ã£o.
-   **Interface de Pergunta FlexÃ­vel:** FaÃ§a perguntas abertas sobre a imagem.
-   **Pergunta PadrÃ£o AutomÃ¡tica:** Se nenhuma pergunta for feita, uma anÃ¡lise completa padrÃ£o Ã© solicitada para facilitar o uso.
-   **UI Moderna e Responsiva:** Interface limpa, organizada em colunas e esteticamente agradÃ¡vel.
-   **Backend Modular:** Arquitetura de software bem estruturada, separando a interface (UI), a lÃ³gica de orquestraÃ§Ã£o (pipeline) e as estratÃ©gias de IA.


## ğŸ› ï¸ Tecnologias Utilizadas

-   **Frontend:** [Streamlit](https://streamlit.io/)
-   **Backend:** [Python 3.9+](https://www.python.org/)
-   **OrquestraÃ§Ã£o de IA:** [LangChain](https://www.langchain.com/)
-   **Modelo de IA:** [OpenAI GPT-4o](https://openai.com/index/hello-gpt-4o/) (ou outro modelo multimodal)
-   **Gerenciamento de DependÃªncias:** `pip` e `requirements.txt`


## ğŸ“‚ Estrutura do Projeto

A estrutura do projeto foi projetada para ser modular e escalÃ¡vel, seguindo princÃ­pios de separaÃ§Ã£o de responsabilidades.

```
MULTIMODAL-ASSISTANT/
â”œâ”€â”€ .venv/                   # Ambiente virtual
â”œâ”€â”€ pipeline/
â”‚   â””â”€â”€ multimodal_pipeline.py # Orquestra o fluxo principal da anÃ¡lise
â”œâ”€â”€ strategies/
â”‚   â”œâ”€â”€ llms/                  # (Opcional) Para lÃ³gicas de texto puro
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â””â”€â”€ base_prompt.py     # Centraliza os prompts do sistema para a IA
â”‚   â””â”€â”€ vision/
â”‚       â””â”€â”€ openai_vision.py   # Implementa a chamada para a API de visÃ£o da OpenAI
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logger.py            # ConfiguraÃ§Ã£o do logger
â”‚   â””â”€â”€ validators.py        # (Opcional) Para validaÃ§Ãµes de dados
â”œâ”€â”€ .env                     # Arquivo para suas chaves de API (NÃƒO versionar)
â”œâ”€â”€ .gitignore               # Arquivos e pastas a serem ignorados pelo Git
â”œâ”€â”€ app.py                   # Arquivo principal da interface Streamlit
â”œâ”€â”€ config.py                # Carrega as configuraÃ§Ãµes do .env
â””â”€â”€ requirements.txt         # Lista de dependÃªncias do projeto
```


## âš™ï¸ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

Siga estes passos para configurar e executar o projeto em sua mÃ¡quina local.

### 1. PrÃ©-requisitos

-   Python 3.9 ou superior
-   Git

### 2. Clone o RepositÃ³rio

```bash
git clone https://github.com/EstevesMarques/multimodal-assistant.git
cd multimodal-assistant
```

### 3. Crie e Ative um Ambiente Virtual

Ã‰ uma boa prÃ¡tica isolar as dependÃªncias do projeto.

-   **No Windows:**
    ```bash
    python -m venv .venv
    .\.venv\Scripts\activate
    ```

-   **No macOS/Linux:**
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate
    ```

### 4. Instale as DependÃªncias

Crie um arquivo `requirements.txt` com o seguinte conteÃºdo:

```txt
streamlit
langchain-openai
python-dotenv
pillow
```

Em seguida, instale as dependÃªncias:

```bash
pip install -r requirements.txt
```

### 5. Configure suas VariÃ¡veis de Ambiente

VocÃª precisarÃ¡ de uma chave de API da OpenAI.

a. Crie um arquivo chamado `.env` na raiz do projeto.

b. Adicione sua chave de API a este arquivo:

```
# .env
OPENAI_API_KEY=sk-proj-......
OPENAI_LLM_MODEL=gpt-4o-mini-2024-07-18
OPENAI_COMPLETION_URL=https://api.openai.com/v1/chat/completions
OPENAI_TEMPERATURE=0.1
```

*O arquivo `.gitignore` jÃ¡ estÃ¡ configurado para ignorar o `.env`, garantindo que sua chave nÃ£o seja enviada para o repositÃ³rio.*


## ğŸš€ Como Executar

Com o ambiente virtual ativado e as dependÃªncias instaladas, inicie a aplicaÃ§Ã£o Streamlit com o seguinte comando:

```bash
streamlit run app.py
```

A aplicaÃ§Ã£o abrirÃ¡ automaticamente em seu navegador padrÃ£o no endereÃ§o `http://localhost:8501`.


## ğŸ§  Como Funciona o Fluxo de AnÃ¡lise

1.  **Interface (`app.py`):** O usuÃ¡rio envia uma imagem (upload ou cÃ¢mera) e digita uma pergunta (ou deixa em branco para a pergunta padrÃ£o).
2.  **Acionamento:** Ao clicar em "Analisar", o `app.py` chama a funÃ§Ã£o `process_image_and_question` do nosso pipeline.
3.  **Pipeline (`multimodal_pipeline.py`):** Esta camada atua como um orquestrador. Ela recebe os dados brutos, valida-os e direciona para a estratÃ©gia de visÃ£o correta.
4.  **EstratÃ©gia de VisÃ£o (`openai_vision.py`):**
    -   A imagem (em bytes) Ã© codificada para o formato base64.
    -   O prompt do sistema (de `base_prompt.py`) Ã© recuperado para dar instruÃ§Ãµes Ã  IA.
    -   Usando LangChain, uma mensagem multimodal Ã© montada, contendo as instruÃ§Ãµes, a pergunta do usuÃ¡rio e a imagem.
    -   Uma chamada Ã© feita para a API da OpenAI.
5.  **Retorno:** A resposta em texto/markdown gerada pela IA flui de volta pelo mesmo caminho atÃ© ser exibida de forma elegante na interface do `app.py`.


## ğŸ”® PossÃ­veis Melhorias Futuras

-   [ ] **HistÃ³rico de AnÃ¡lises:** Salvar as anÃ¡lises do usuÃ¡rio em um banco de dados local (SQLite) ou na nuvem.
-   [ ] **Suporte a MÃºltiplos Modelos:** Adicionar uma opÃ§Ã£o para escolher entre diferentes modelos de IA (ex: Google Gemini).
-   [ ] **Dashboard Nutricional:** Criar grÃ¡ficos e visualizaÃ§Ãµes para acompanhar a ingestÃ£o de nutrientes ao longo do tempo.
-   [ ] **AutenticaÃ§Ã£o de UsuÃ¡rio:** Implementar um sistema de login para que cada usuÃ¡rio tenha seu prÃ³prio histÃ³rico.

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ distribuÃ­do sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.